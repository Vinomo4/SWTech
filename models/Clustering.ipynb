{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "# Define path with .py codes containing functions used in this script\n",
    "os.getcwd()\n",
    "os.chdir( '../src/features')\n",
    "# Import useful functions for this script  \n",
    "from tracking import track\n",
    "\n",
    "\n",
    "os.chdir( '../models')\n",
    "from F_Models import save_plot,transform_dataset,WCSS_and_Elbow_Method,define_num_clusters,compute_PCA,compute_UMAP,plot_clusters\n",
    "\n",
    "track(\"-\"*25 + \"CLUSTERING\" + \"-\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define path to data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Defining path to data files\")\n",
    "\n",
    "# Define base path to data files\n",
    "path = '../../temp_data/'\n",
    "\n",
    "# Define path to the preprocesseded dataset that will be used in this script\n",
    "path_preprocessed_data = path + 'model_data.csv'\n",
    "\n",
    "# Ensure the input file exists\n",
    "assert os.path.isfile(path_preprocessed_data), f'{path_preprocessed_data} not found. Is it a file?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read table with preprocesseded data that will be used in this script\n",
    "track(\"Reading preprocessed data\")\n",
    "preprocessed_data = pd.read_csv(path_preprocessed_data)\n",
    "track(\"Finished reading preprocessed data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column containing the name of the author\n",
    "data = preprocessed_data.loc[:, preprocessed_data.columns != 'author']\n",
    "track(\"Author column was dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_cols = ['author_timezone', 'commit_message', 'n_commits', 'n_projects_c', 'complexity', 'cognitive_complexity', 'duplicated_blocks', 'duplicated_files', \n",
    "                'duplicated_lines_density', 'open_issues', 'files', 'comment_lines_density', 'n_measures', 'n_projects_m', 'effort', 'message']\n",
    "\n",
    "#quality_cols = ['violations', 'blocker_violations', 'critical_violations', 'major_violations', 'minor_violations', 'n_projects_i']\n",
    "# info_violations, 'sqale_debt_ratio', 'code_smells', 'bugs', 'reliability_rating', 'vulnerabilities', 'security_rating', \n",
    "#'blocker','critical', 'info', 'major', 'minor', 'issue_code_length', 'n_issues'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means with normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Starting mix max scaling of data\")\n",
    "# Min max scaling of data\n",
    "track(\"Finished mix max scaling of data\")\n",
    "min_max_data = transform_dataset(data[cluster_cols],type=\"min_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of clusters and which cluster is every author\n",
    "track(\"Starting WCSS and Elbow method for choosing the number of clusters\")\n",
    "clusters_none , number_of_clusters_none , silhouette_none = define_num_clusters(min_max_data,min_k=4, max_k=12, method=\"Normalized_data\")\n",
    "track(\"Finished WCSS and Elbow method for choosing the number of clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means with PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA\n",
    "track(\"Starting to compute PCA\")\n",
    "PCA_data = compute_PCA(min_max_data, min_var=0.95)\n",
    "track(\"Finished computing PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of clusters and which cluster is every author\n",
    "track(\"Starting WCSS and Elbow method for choosing the number of clusters\")\n",
    "clusters_PCA, number_of_clusters_PCA, silhouette_PCA = define_num_clusters(PCA_data,min_k=4,max_k=12,method =\"PCA\")\n",
    "track(\"Finished WCSS and Elbow method for choosing the number of clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means with UMAP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Starting standardization of data\")\n",
    "# Standardization of data\n",
    "track(\"Finished standardization of data\")\n",
    "standardized_data = transform_dataset(data[cluster_cols],type=\"standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute UMAP\n",
    "track(\"Starting to compute UMAP\")\n",
    "UMAP_data = compute_UMAP(standardized_data,n_neighbors=40,min_dist=0.01,n_components=10)\n",
    "track(\"Finished computing UMAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of clusters and which cluster is every author\n",
    "track(\"Starting WCSS and Elbow method for choosing the number of clusters\")\n",
    "clusters_UMAP, number_of_clusters_UMAP, silhouette_UMAP = define_num_clusters(UMAP_data,min_k=4,max_k=12,method ='UMAP')\n",
    "track(\"Finished WCSS and Elbow method for choosing the number of clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if silhouette_PCA > silhouette_UMAP and silhouette_PCA > silhouette_none:\n",
    "    track(\"Computing PCA plot with K-Means clusters\")\n",
    "    plot_clusters(clusters_PCA, min_max_data,standardized_data,\"PCA\")\n",
    "    preprocessed_data['clusters'] = clusters_PCA\n",
    "elif silhouette_PCA < silhouette_UMAP and silhouette_UMAP > silhouette_none:\n",
    "    track(\"Computing UMAP plot with K-Means clusters\")\n",
    "    plot_clusters(clusters_UMAP, min_max_data, standardized_data,\"UMAP\")\n",
    "    preprocessed_data['clusters'] = clusters_UMAP\n",
    "else:\n",
    "    track(\"Computing PCA and UMAP plots with K-Means clusters\")\n",
    "    plot_clusters(clusters_none, min_max_data, standardized_data,\"None\")\n",
    "    preprocessed_data['clusters'] = clusters_none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data with clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, the final dataframe with the cluster variable is written in the suitable folder.\n",
    "\n",
    "try: os.mkdir(\"../../temp_data/\")\n",
    "except: pass\n",
    "preprocessed_data.to_csv(\"../../temp_data/model_data_with_clusters.csv\", index_label = \"author\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Metric calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'quality_rating' (Series)\n"
     ]
    }
   ],
   "source": [
    "track(\"Creating quality rating\")\n",
    "quality_rating_data = preprocessed_data.groupby('clusters').agg({\n",
    "    'violations': 'sum',\n",
    "                            'blocker_violations': 'sum',\n",
    "                            'critical_violations': 'sum',\n",
    "                            'major_violations': 'sum',\n",
    "                            'minor_violations': 'sum',\n",
    "                            'blocker': 'sum',\n",
    "                            'critical': 'sum',\n",
    "                            'major': 'sum',\n",
    "                            'minor': 'sum',\n",
    "                            'code_smells': 'sum',\n",
    "                            'bugs': 'sum',\n",
    "                            'vulnerabilities': 'sum',\n",
    "                            'sqale_debt_ratio': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate ponderated mean of the violations variables\n",
    "violations = quality_rating_data[[\"blocker_violations\", \"critical_violations\", \"major_violations\", \"minor_violations\"]] \n",
    "violations = violations*[0.5, 0.4, 0.07, 0.03]\n",
    "violations = np.sum(violations, axis=1)\n",
    "# Calculate ponderated mean of the severity issues variables\n",
    "severity = quality_rating_data[[\"blocker\", \"critical\", \"major\", \"minor\"]]\n",
    "severity = severity*[0.5, 0.4, 0.15, 0.05]\n",
    "severity = np.sum(severity, axis=1)\n",
    "# Add violations and severity columns in the dataset\n",
    "quality_rating_data['violations'] = violations\n",
    "quality_rating_data['severity'] = severity\n",
    "# Discard all types of violations and severity issues columns\n",
    "quality_rating_data = quality_rating_data.drop([\"clusters\", \"blocker_violations\", \"critical_violations\", \"major_violations\", \"minor_violations\", \"blocker\", \"critical\", \"major\", \"minor\"], axis=1)\n",
    "quality_rating_data\n",
    "# compute the mean of each cluster\n",
    "quality_rating_data = quality_rating_data[[\"violations\", \"code_smells\",\t\"bugs\",\t\"vulnerabilities\",\t\"severity\"]]\n",
    "quality_rating_data\n",
    "suma = np.sum(quality_rating_data, axis=1)\n",
    "\n",
    "total = np.sum(suma)\n",
    "quality_rating = 1 - (suma/total)\n",
    "%store quality_rating\n",
    "track(\"Finished creating quality rating\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
