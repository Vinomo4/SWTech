{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and packages\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Define path with .py codes containing functions used in this script\n",
    "os.getcwd()\n",
    "os.chdir( '../src/features')\n",
    "\n",
    "# Import useful functions for this script  \n",
    "from tracking import track\n",
    "import matplotlib.pyplot as plt\n",
    "os.chdir( '../models')\n",
    "from F_validation import plot_quality_metrics\n",
    "\n",
    "track(\"-\"*25 + \"VALIDATION\" + \"-\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data\n",
    "#### Define path to data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Defining path to data files\")\n",
    "\n",
    "# Define base path to data files\n",
    "path = '../../temp_data/'\n",
    "\n",
    "# Define path to the model_data_with_clusters table\n",
    "path_clusters_data = path + 'model_data_with_clusters.csv'\n",
    "\n",
    "# Ensure the input file exist\n",
    "assert os.path.isfile(path_clusters_data), f'{path_clusters_data} not found. Is it a file?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Reading files\")\n",
    "\n",
    "# Read clusterized data \n",
    "clusterized_data = pd.read_csv(path_clusters_data)\n",
    "\n",
    "track(\"Finished reading files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Creating average quality dataset\")\n",
    "average_quality = (clusterized_data.groupby('clusters').agg(blocker_violations_mean = ('blocker_violations', 'mean'),\n",
    "                                          blocker_violations_std = ('blocker_violations', 'std'),\n",
    "                                          critical_violations_mean = ('critical_violations', 'mean'),\n",
    "                                          critical_violations_std = ('critical_violations', 'std'),\n",
    "                                          major_violations_mean = ('major_violations', 'mean'),\n",
    "                                          major_violations_std = ('major_violations', 'std'),\n",
    "                                          minor_violations_mean = ('minor_violations', 'mean'),\n",
    "                                          minor_violations_std = ('minor_violations', 'std'),\n",
    "                                          code_smells_mean = ('code_smells', 'mean'),\n",
    "                                          code_smells_std = ('code_smells', 'std'),\n",
    "                                          bugs_mean = ('bugs', 'mean'),\n",
    "                                          bugs_std = ('bugs', 'std'),\n",
    "                                          vulnerabilities_mean = ('vulnerabilities', 'mean'),\n",
    "                                          vulnerabilities_std = ('vulnerabilities', 'std'),\n",
    "                                          blocker_mean = ('blocker', 'mean'),\n",
    "                                          blocker_std = ('blocker', 'std'),\n",
    "                                          critical_mean = ('critical', 'mean'),\n",
    "                                          critical_std = ('critical', 'std'),\n",
    "                                          major_mean = ('major', 'mean'),\n",
    "                                          major_std = ('major', 'std'),\n",
    "                                          minor_mean = ('minor', 'mean'),\n",
    "                                          minor_std = ('minor', 'std'))).reset_index()\n",
    "track(\"Finished creating average quality dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'track(\"Creating quality rating\")\\nquality_rating_data = clusterized_data.groupby(\\'clusters\\').agg({\\n    \\'violations\\': \\'sum\\',\\n                            \\'blocker_violations\\': \\'sum\\',\\n                            \\'critical_violations\\': \\'sum\\',\\n                            \\'major_violations\\': \\'sum\\',\\n                            \\'minor_violations\\': \\'sum\\',\\n                            \\'blocker\\': \\'sum\\',\\n                            \\'critical\\': \\'sum\\',\\n                            \\'major\\': \\'sum\\',\\n                            \\'minor\\': \\'sum\\',\\n                            \\'code_smells\\': \\'sum\\',\\n                            \\'bugs\\': \\'sum\\',\\n                            \\'vulnerabilities\\': \\'sum\\',\\n                            \\'sqale_debt_ratio\\': \\'mean\\',\\n}).reset_index()\\n\\n# Calculate ponderated mean of the violations variables\\nviolations = quality_rating_data[[\"blocker_violations\", \"critical_violations\", \"major_violations\", \"minor_violations\"]] \\nviolations = violations*[0.5, 0.4, 0.07, 0.03]\\nviolations = np.sum(violations, axis=1)\\n# Calculate ponderated mean of the severity issues variables\\nseverity = quality_rating_data[[\"blocker\", \"critical\", \"major\", \"minor\"]]\\nseverity = severity*[0.5, 0.4, 0.15, 0.05]\\nseverity = np.sum(severity, axis=1)\\n# Add violations and severity columns in the dataset\\nquality_rating_data[\\'violations\\'] = violations\\nquality_rating_data[\\'severity\\'] = severity\\n# Discard all types of violations and severity issues columns\\nquality_rating_data = quality_rating_data.drop([\"clusters\", \"blocker_violations\", \"critical_violations\", \"major_violations\", \"minor_violations\", \"blocker\", \"critical\", \"major\", \"minor\"], axis=1)\\nquality_rating_data\\n# compute the mean of each cluster\\nquality_rating_data = quality_rating_data[[\"violations\", \"code_smells\",\\t\"bugs\",\\t\"vulnerabilities\",\\t\"severity\"]]\\nquality_rating_data\\nsuma = np.sum(quality_rating_data, axis=1)\\n\\ntotal = np.sum(suma)\\nquality_rating = 1 - (suma/total)\\ntrack(\"Finished creating quality rating\")'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''track(\"Creating quality rating\")\n",
    "quality_rating_data = clusterized_data.groupby('clusters').agg({\n",
    "    'violations': 'sum',\n",
    "                            'blocker_violations': 'sum',\n",
    "                            'critical_violations': 'sum',\n",
    "                            'major_violations': 'sum',\n",
    "                            'minor_violations': 'sum',\n",
    "                            'blocker': 'sum',\n",
    "                            'critical': 'sum',\n",
    "                            'major': 'sum',\n",
    "                            'minor': 'sum',\n",
    "                            'code_smells': 'sum',\n",
    "                            'bugs': 'sum',\n",
    "                            'vulnerabilities': 'sum',\n",
    "                            'sqale_debt_ratio': 'mean',\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate ponderated mean of the violations variables\n",
    "violations = quality_rating_data[[\"blocker_violations\", \"critical_violations\", \"major_violations\", \"minor_violations\"]] \n",
    "violations = violations*[0.5, 0.4, 0.07, 0.03]\n",
    "violations = np.sum(violations, axis=1)\n",
    "# Calculate ponderated mean of the severity issues variables\n",
    "severity = quality_rating_data[[\"blocker\", \"critical\", \"major\", \"minor\"]]\n",
    "severity = severity*[0.5, 0.4, 0.15, 0.05]\n",
    "severity = np.sum(severity, axis=1)\n",
    "# Add violations and severity columns in the dataset\n",
    "quality_rating_data['violations'] = violations\n",
    "quality_rating_data['severity'] = severity\n",
    "# Discard all types of violations and severity issues columns\n",
    "quality_rating_data = quality_rating_data.drop([\"clusters\", \"blocker_violations\", \"critical_violations\", \"major_violations\", \"minor_violations\", \"blocker\", \"critical\", \"major\", \"minor\"], axis=1)\n",
    "quality_rating_data\n",
    "# compute the mean of each cluster\n",
    "quality_rating_data = quality_rating_data[[\"violations\", \"code_smells\",\t\"bugs\",\t\"vulnerabilities\",\t\"severity\"]]\n",
    "quality_rating_data\n",
    "suma = np.sum(quality_rating_data, axis=1)\n",
    "\n",
    "total = np.sum(suma)\n",
    "quality_rating = 1 - (suma/total)\n",
    "track(\"Finished creating quality rating\")'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Creating plots of quality metrics\")\n",
    "# There will be three different plots. It is necessary to create the groups of the variables to be printed in the same plot \n",
    "violation_metrics = [\"blocker_violations\", \"critical_violations\", \"major_violations\", \"minor_violations\"]\n",
    "severity_metrics = [\"blocker\", \"critical\", \"major\", \"minor\"]\n",
    "other_metrics = [\"code_smells\", \"bugs\", \"vulnerabilities\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the quality_rating variable created in the Models.ipynb\n",
    "%store -r quality_rating\n",
    "# Order the quality rating from the best cluster to the words one. It allows to compare the values from the cluster with highest quality rating to the lowest one.\n",
    "average_quality['quality_rating'] = quality_rating\n",
    "sorted_average_quality = average_quality.sort_values(by=['quality_rating'],ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quality_metrics(sorted_average_quality, violation_metrics, [0, 5000, 10000, 15000, 20000, 25000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quality_metrics(sorted_average_quality, severity_metrics, [-1000, -500, 0, 500, 1000, 1500, 2000, 2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_quality_metrics(sorted_average_quality, other_metrics, [0, 5000, 10000, 15000, 20000, 25000, 30000, 35000, 40000, 45000])\n",
    "track(\"Finished creating plots of quality metrics\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
