{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "#### Import labraries and packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# Import libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import collections\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define path with .py codes containing functions used in this script\n",
    "os.getcwd()\n",
    "os.chdir( '../src/features')\n",
    "\n",
    "# Import useful functions for this script  \n",
    "from tracking import track\n",
    "from basic_statistics import save_outputs, describe_variables, bar_plot, plot_histogram, plot_correlations\n",
    "from data_quality import initialize_doc, write_test_results, write_extra_info, check_PK, check_NAs, check_missing_authors, check_dates, check_FK, check_duplicates\n",
    "\n",
    "# Initializing the tracking file\n",
    "if os.path.exists('../../reports/tracking/track.txt'):\n",
    "    os.remove('../../reports/tracking/track.txt')\n",
    "track(\"-\"*25 + \"DATA UNDERSTANDING\" + \"-\"*25)\n",
    "track(\"Finished importing libraries\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Hola\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Reading data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define path to data files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track(\"Defining path to data files\")\n",
    "\n",
    "# Define base path to data files\n",
    "path1 = '../../data/raw/'\n",
    "path2 = \"../data/raw/\"\n",
    "\n",
    "\n",
    "# Define path to the tables that will be used in this project\n",
    "# These are: GIT_COMMITS_CHANGES, GIT_COMMITS, JIRA_ISSUES, SONAR_ANALYSIS, SONAR_ISSUES, and SONAR_MEASURES tables\n",
    "path_git_commits_changes = path2 + 'GIT_COMMITS_CHANGES.csv'\n",
    "path_git_commits = path1 + 'GIT_COMMITS.csv'\n",
    "path_jira_issues = path1 + 'JIRA_ISSUES.csv'\n",
    "path_sonar_analysis = path1 + 'SONAR_ANALYSIS.csv'\n",
    "path_sonar_issues = path1 + 'SONAR_ISSUES.csv'\n",
    "path_sonar_measures = path1 + 'SONAR_MEASURES.csv'\n",
    "\n",
    "# Ensure the input file exist\n",
    "assert os.path.isfile(\"../\" + path_git_commits_changes), f'{path_git_commits_changes} not found. Is it a file?'\n",
    "assert os.path.isfile(path_git_commits), f'{path_git_commits} not found. Is it a file?'\n",
    "assert os.path.isfile(path_jira_issues), f'{path_jira_issues} not found. Is it a file?'\n",
    "assert os.path.isfile(path_sonar_analysis), f'{path_sonar_analysis} not found. Is it a file?'\n",
    "assert os.path.isfile(path_sonar_issues), f'{path_sonar_issues} not found. Is it a file?'\n",
    "assert os.path.isfile(path_sonar_measures), f'{path_sonar_measures} not found. Is it a file?'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Read the files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track(\"Reading files\")\n",
    "\n",
    "# Read GIT_COMMITS_CHANGES, GIT_COMMITS, JIRA_ISSUES, SONAR_ANALYSIS, SONAR_ISSUES, and SONAR_MEASURES tables\n",
    "# The first table is read with spark because it if not, it could be that the kernel was interrupted\n",
    "git_commits_changes = spark.read.csv(path_git_commits_changes, header=True).toPandas()\n",
    "git_commits = pd.read_csv(path_git_commits)\n",
    "jira_issues = pd.read_csv(path_jira_issues)\n",
    "sonar_analysis = pd.read_csv(path_sonar_analysis)\n",
    "sonar_issues = pd.read_csv(path_sonar_issues)\n",
    "sonar_measures = pd.read_csv(path_sonar_measures)\n",
    "\n",
    "track(\"Finished reading files\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Select attributes of interest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track(\"Defining attributes of intereset for each dataframe...\")\n",
    "\n",
    "# Define attributes of interest for each table.\n",
    "# The other attributes not in the list will be excluded because they are considered irrelevant for the purpose of this project\n",
    "git_commits_changes_names = ['COMMIT_HASH','DATE','LINES_ADDED','LINES_REMOVED']\n",
    "git_commits_names = ['PROJECT_ID','COMMIT_HASH','AUTHOR','AUTHOR_DATE','AUTHOR_TIMEZONE','COMMIT_MESSAGE']\n",
    "jira_issues_names = ['HASH']\n",
    "sonar_analysis_names = ['PROJECT_ID','ANALYSIS_KEY','REVISION']\n",
    "sonar_issues_names = ['CREATION_ANALYSIS_KEY','SEVERITY','STATUS','EFFORT','MESSAGE','START_LINE','END_LINE','CLOSE_ANALYSIS_KEY']\n",
    "sonar_measures_names = ['analysis_key','complexity' ,'cognitive_complexity', 'coverage', 'duplicated_blocks', 'duplicated_files', \n",
    "                        'duplicated_lines_density', 'violations','blocker_violations','critical_violations','major_violations','minor_violations','info_violations','false_positive_issues','open_issues','reopened_issues','confirmed_issues', 'sqale_debt_ratio','code_smells','bugs','reliability_rating','vulnerabilities','security_rating','files', 'comment_lines_density']           "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Select attributes of interest according to the defined lists above\n",
    "git_commits_changes = git_commits_changes[git_commits_changes_names]\n",
    "git_commits = git_commits[git_commits_names]\n",
    "jira_issues = jira_issues[jira_issues_names]\n",
    "sonar_analysis = sonar_analysis[sonar_analysis_names]\n",
    "sonar_issues = sonar_issues[sonar_issues_names]\n",
    "sonar_measures = sonar_measures[sonar_measures_names]\n",
    "\n",
    "track(\"Finished selecting attributes of intereset for each dataframe\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Define numerical types"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track(\"Defining numerical types...\")\n",
    "\n",
    "# Define list with possible numerical types in the dataframes\n",
    "# This list will be used for selecting or excluding numerical variables\n",
    "numerical = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "\n",
    "track(\"Finished defining numercial types\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Reformat columns"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track(\"Modifying wrongly formatted columns...\")\n",
    "\n",
    "# Modify wrongly formated columns (LINES_ADDED and LINES_REMOVED in GIT_COMMITS_CHANGES)\n",
    "# Changing column types from object to float64\n",
    "git_commits_changes[[\"LINES_ADDED\",\"LINES_REMOVED\"]] = git_commits_changes[[\"LINES_ADDED\",\"LINES_REMOVED\"]].apply(pd.to_numeric)\n",
    "\n",
    "track(\"Finished modifying wrongly formatted columns\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Describe data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Type of variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('-------------------------------------------------------------------------------')\n",
    "track(\"Defining attributes types and saving results...\")\n",
    "\n",
    "track(\"--> Git commits changes\")\n",
    "save_outputs(git_commits_changes[git_commits_changes_names].dtypes,\"type_variables\",None,'git_commits_changes')\n",
    "\n",
    "track(\"--> Git commits\")\n",
    "save_outputs(git_commits[git_commits_names].dtypes,\"type_variables\",None,'git_commits')\n",
    "\n",
    "track(\"--> Jira issues\")\n",
    "save_outputs(jira_issues[jira_issues_names].dtypes,\"type_variables\",None,'jira_issues')\n",
    "\n",
    "track(\"--> Sonar analysis\")\n",
    "save_outputs(sonar_analysis[sonar_analysis_names].dtypes,\"type_variables\",None,'sonar_analysis')\n",
    "\n",
    "track(\"--> Sonar issues\")\n",
    "save_outputs(sonar_issues[sonar_issues_names].dtypes,\"type_variables\",None,'sonar_issues')\n",
    "\n",
    "track(\"--> Sonar measures\")\n",
    "save_outputs(sonar_measures[sonar_measures_names].dtypes,\"type_variables\",None,'sonar_measures')\n",
    "\n",
    "track(\"Finished defining types of variables\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Computing basic statistics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track(\"Starting computing basic statistics for the variables of interest for each table\")\n",
    "\n",
    "# Print basic statistics for the variables of interest of each table\n",
    "# For the continuous variables, the metrics studied are: count of non-null observations, mean of values, minimum value, \n",
    "# maximum value, and 25%, 50%, 75% percentiles. \n",
    "# While for the categorical variables, these are count of non-null observations, number of non-null observations, \n",
    "# number of unique classes, top class with more occurrences, and frequency of occurrence of the top class.\n",
    "\n",
    "track(\"--> Git commits changes\")\n",
    "describe_variables(git_commits_changes,numerical,'git_commits')\n",
    "\n",
    "track(\"--> Git commits \")\n",
    "describe_variables(git_commits,numerical,'git_commits')\n",
    "\n",
    "track(\"--> Jira issues \")\n",
    "describe_variables(jira_issues,numerical,'jira_issues')\n",
    "\n",
    "track(\"--> Sonar analysis \")\n",
    "describe_variables(sonar_analysis,numerical,'sonar_analysis')\n",
    "\n",
    "track(\"--> Sonar issues \")\n",
    "describe_variables(sonar_issues,numerical,'sonar_issues')\n",
    "\n",
    "track(\"--> Sonar measures \")\n",
    "describe_variables(sonar_measures,numerical,'sonar_measures')\n",
    "\n",
    "track('Finished computing and saving the type of all variables')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Bar plots "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('Starting plotting bar plots')\n",
    "# Plot bar plots for the categorical variables with few values\n",
    "# These are PROJECT_ID from SONAR_ANALYSIS table and SEVERITY and STATUS from SONAR_ISSUES table\n",
    "track('--> Sonar analysis')\n",
    "bar_plot(sonar_analysis[\"PROJECT_ID\"],sonar_analysis,True,'sonar_analysis')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('--> Sonar issues')\n",
    "bar_plot(sonar_issues[\"SEVERITY\"],sonar_issues,False,'sonar_issues')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('--> Sonar issues')\n",
    "bar_plot(sonar_issues[\"STATUS\"],sonar_issues,False,'sonar_issues')\n",
    "\n",
    "track('Finished computing and saving the bar plots')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Histogram plots "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('Starting plotting histograms')\n",
    "# Plot histogram of all numeric variables in dataframe\n",
    "\n",
    "track(\"--> Git commits changes\")\n",
    "#plot_histogram(git_commits_changes,numerical,'git_commits_changes')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('--> Git commits')\n",
    "plot_histogram(git_commits,numerical,'git_commits')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('--> Jira issues')\n",
    "plot_histogram(jira_issues,numerical,'jira_issues')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('--> Sonar analysis')\n",
    "plot_histogram(sonar_analysis,numerical,'sonar_analysis')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('--> Sonar issues')\n",
    "plot_histogram(sonar_issues,numerical,'sonar_issues')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('--> Sonar measures')\n",
    "plot_histogram(sonar_measures,numerical,'sonar_measures')\n",
    "\n",
    "track('Finishing computing and saving the histograms')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Correlation plots "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('Starting plotting heatmaps of correlations')\n",
    "\n",
    "# Plot heatmap of correlation of all numeric variables in dataframe\n",
    "\n",
    "track('--> Git commits changes')\n",
    "plot_correlations(git_commits_changes,numerical,'git_commits_changes')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('--> Git commits')\n",
    "plot_correlations(git_commits,numerical,'git_commits')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('--> Jira issues')\n",
    "plot_correlations(jira_issues,numerical,'jira_issues')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('--> Sonar analysis')\n",
    "plot_correlations(sonar_analysis,numerical,'sonar_analysis')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('--> Sonar issues')\n",
    "plot_correlations(sonar_issues,numerical,'sonar_issues')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('--> Sonar measures')\n",
    "plot_correlations(sonar_measures,numerical,'sonar_measures')\n",
    "\n",
    "track('Finished computing and saving the heatmap plots with correlations')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Checking data quality\n",
    "\n",
    "In this section, an evaluation of the overall quality of the data for each table used in the project is performed."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SONAR_MEASURES**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "track('-------------------------------------------------------------------------------')\n",
    "track('Starting checking data quality')\n",
    "\n",
    "## SONAR MEASURES\n",
    "track('--> Sonar measures')\n",
    "sonar_measures_txt = \"sonar_measures_quality_analysis.txt\"\n",
    "print(initialize_doc(sonar_measures_txt,\"Sonar measures\"),end = \"\")\n",
    "t1_result,fk_violations = check_FK(sonar_measures,'analysis_key',sonar_analysis,'ANALYSIS_KEY')\n",
    "print(write_test_results(sonar_measures_txt,\"FK\",[t1_result,fk_violations]),end = \"\")\n",
    "t2_result,_ = check_NAs(sonar_measures,sonar_measures_names)\n",
    "print(write_test_results(sonar_measures_txt,\"NA\",t2_result),end = \"\")\n",
    "t3_result = check_duplicates(sonar_measures)\n",
    "print(write_test_results(sonar_measures_txt,\"DUPLICATED\",t3_result),end = \"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SONAR_ISSUES** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## SONAR ISSUES\n",
    "track('--> Sonar issues')\n",
    "sonar_issues_txt = \"sonar_issues_quality_analysis.txt\"\n",
    "print(initialize_doc(sonar_issues_txt,\"Sonar issues\"),end = \"\")\n",
    "t1_result,NA_columns = check_NAs(sonar_issues, sonar_issues_names)\n",
    "print(write_test_results(sonar_issues_txt,\"NA\",t1_result),end = \"\")\n",
    "t2_result = check_duplicates(sonar_issues)\n",
    "print(write_test_results(sonar_issues_txt,\"DUPLICATED\",t2_result),end = \"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Checking if all the rows that don't have a starting line also don't have an end line."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "extra_intro = \"As the % of START_LINE is equal to the one in the END_LINE, we will check if the rows match.\"\n",
    "aux_start_end = sonar_issues[sonar_issues.START_LINE.notna()]\n",
    "if aux_start_end.END_LINE.isna().sum() != 0:\n",
    "    result = \"There are rows with an START_LINE value that are missing the END_LINE value\"\n",
    "else: result = \"All the rows match!\"\n",
    "write_extra_info(sonar_issues_txt,\"Extra check: missing values\",result,extra_intro)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Checking if all the rows with missing CLOSE_ANALYSIS_KEY have STATUS = OPEN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "extra_intro = \"We will check if all the rows with missing CLOSE_ANALYSIS_KEY have STATUS = OPEN.\"\n",
    "aux_cak = sonar_issues[sonar_issues.CLOSE_ANALYSIS_KEY.isna()]\n",
    "if sum(aux_cak.STATUS == \"OPEN\")!= len(aux_cak):\n",
    "    result = \"There are \" + str(len(aux_cak) - sum(aux_cak.STATUS == \"OPEN\"))+ \" closed issues without closing analysis keys.\"\n",
    "else: result = \"All the rows match!\"\n",
    "write_extra_info(sonar_issues_txt,\"Extra check: missing values\",result,extra_intro)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**SONAR_ANALYSIS**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## SONAR ANALYSIS\n",
    "track('--> Sonar analysis')\n",
    "sonar_analysis_txt = \"sonar_analyisis_quality_analysis.txt\"\n",
    "print(initialize_doc(sonar_analysis_txt,\"Sonar analysis\"),end = \"\")\n",
    "t1_result = check_PK(sonar_analysis,'ANALYSIS_KEY')\n",
    "print(write_test_results(sonar_analysis_txt,\"PK\",t1_result),end = \"\")\n",
    "t2_result,fk_violations = check_FK(sonar_analysis,'REVISION',git_commits,'COMMIT_HASH')\n",
    "print(write_test_results(sonar_analysis_txt,\"FK\",[t2_result,fk_violations]),end = \"\")\n",
    "t3_result = check_duplicates(sonar_analysis)\n",
    "print(write_test_results(sonar_analysis_txt,\"DUPLICATED\",t3_result),end = \"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As problems in the PK were observed, a further quality examination has been performed. After a quick exploration, we noticed the presence of null values as PK in the analysis. Thus, we wanted to evaluate whether this was the only present problem and the number of rows affected by this phenomenon."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "extra_intro = \"Further exploration on the PK repeated values\"\n",
    "if len(sonar_analysis[sonar_analysis['ANALYSIS_KEY'].notna()]) == len(sonar_analysis[sonar_analysis['ANALYSIS_KEY'].notna()].ANALYSIS_KEY.unique()):\n",
    "    result = \"All the repeated values on the PK variable are NAs.\" \n",
    "    result = result + \"\\n\"+\" \"*3 +\"The percentage of NA PK is: \"+ str(round(sum(sonar_analysis.ANALYSIS_KEY.isna())/len(sonar_analysis)*100,3))+\"%.\"\n",
    "else: result = \"The repeated values on the PK column are not NAs.\"\n",
    "write_extra_info(sonar_analysis_txt,\"Extra check: pk violation\",result,extra_intro)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**[EXTRA]**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "extra_intro = \"After reporting the FK problem to Davide, he told us that he wanted to observe\" \\\n",
    "              \"wheteher all the rows that contained such problems were related to a an specific\" \\\n",
    "              \"project or affected to serveral ones. Thus, we performed such analysis:\"\n",
    "projects_with_fk_problems = [sonar_analysis[sonar_analysis.REVISION == x].PROJECT_ID for x in fk_violations]\n",
    "projects_with_fk_problems = collections.Counter([x.values[0] for x in projects_with_fk_problems])\n",
    "aux = collections.Counter(sonar_analysis.PROJECT_ID.values)\n",
    "result = \"The % of FK violations per projects are:\\n\" + '\\n'.join(\" \"*6+x+\" â†’ \"+str(round(projects_with_fk_problems[x]/aux[x]*100,4))+\"%\" for x in list(projects_with_fk_problems.keys()))\n",
    "write_extra_info(sonar_analysis_txt,\"Extra check: fk violation\",result,extra_intro)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**JIRA_ISSUES** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## JIRA ISSUES\n",
    "track('--> Jira issues')\n",
    "jira_issues_txt = \"jira_issues_quality_analysis.txt\"\n",
    "print(initialize_doc(jira_issues_txt,\"Jira issues\"),end = \"\")\n",
    "t1_result,fk_violations = check_FK(jira_issues,'HASH',git_commits,'COMMIT_HASH')\n",
    "print(write_test_results(jira_issues_txt,\"FK\",[t1_result,fk_violations]),end = \"\")\n",
    "t2_result = check_duplicates(jira_issues)\n",
    "print(write_test_results(jira_issues_txt,\"DUPLICATED\",t2_result),end = \"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**GIT_COMMITS**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## GIT COMMITS\n",
    "track('--> Git commits')\n",
    "git_commits_txt = \"git_commits_quality_analysis.txt\"\n",
    "print(initialize_doc(git_commits_txt,\"Git commits\"),end = \"\")\n",
    "t1_result = check_PK(git_commits,'COMMIT_HASH')\n",
    "print(write_test_results(git_commits_txt,\"PK\",t1_result),end = \"\")\n",
    "t2_result = check_dates(git_commits,'AUTHOR_DATE',datetime.today(),'1999',1)\n",
    "print(write_test_results(git_commits_txt,\"DATES RANGE\",t2_result),end = \"\")\n",
    "t3_result = check_missing_authors(git_commits,'AUTHOR')\n",
    "print(write_test_results(git_commits_txt,\"MISSING AUTHORS\",t3_result),end = \"\")\n",
    "t4_result = check_duplicates(git_commits)\n",
    "print(write_test_results(git_commits_txt,\"DUPLICATED\",t4_result),end = \"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "**GIT_COMMITS_CHANGES**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## GIT COMMITS CHANGES\n",
    "track('--> Git commits changes')\n",
    "git_commits_changes_txt = \"git_commits_changes_quality_analysis.txt\"\n",
    "print(initialize_doc(git_commits_changes_txt,\"Git commits changes\"),end = \"\")\n",
    "t1_result = check_dates(git_commits_changes,'DATE',datetime.today(),'1999',2)\n",
    "print(write_test_results(git_commits_changes_txt,\"DATES RANGE\",t1_result),end = \"\")\n",
    "t2_result,fk_violations = check_FK(git_commits_changes,'COMMIT_HASH',git_commits,'COMMIT_HASH')\n",
    "print(write_test_results(git_commits_changes_txt,\"FK\",[t2_result,fk_violations]),end = \"\")\n",
    "t3_result,_ = check_NAs(git_commits_changes,git_commits_changes_names)\n",
    "print(write_test_results(git_commits_changes_txt,\"NA\",t3_result),end = \"\")\n",
    "t4_result = check_duplicates(git_commits_changes)\n",
    "print(write_test_results(git_commits_changes_txt,\"DUPLICATED\",t4_result),end = \"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As the percentage of missing values is the same for all the columns, we will check if a row misses one value, misses all."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "extra_intro = \"We will check if the NAs from different columns belong to the same rows.\"\n",
    "aux_na = git_commits_changes[git_commits_changes.COMMIT_HASH.isna()]\n",
    "na_bool = aux_na.isnull().all().all()\n",
    "if na_bool:\n",
    "    result = \"If a row contains a NA value, there are also NAs for the rest of the columns.\"\n",
    "else: result = \"The NAs of the columns do not belong to a same subset of rows.\"\n",
    "write_extra_info(git_commits_changes_txt,\"Extra check: missing values\",result,extra_intro)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}