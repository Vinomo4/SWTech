{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from umap import UMAP\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "# Define path with .py codes containing functions used in this script\n",
    "os.getcwd()\n",
    "os.chdir( '../src/features')\n",
    "# Import useful functions for this script  \n",
    "from tracking import track\n",
    "\n",
    "\n",
    "os.chdir( '../models')\n",
    "from F_Models import save_plot,transform_dataset,WCSS_and_Elbow_Method,define_num_clusters,compute_PCA,compute_UMAP,plot_clusters\n",
    "\n",
    "track(\"-\"*25 + \"CLUSTERING\" + \"-\"*25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define path to data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Defining path to data files\")\n",
    "\n",
    "# Define base path to data files\n",
    "path = '../../temp_data/'\n",
    "\n",
    "# Define path to the preprocesseded dataset that will be used in this script\n",
    "path_preprocessed_data = path + 'model_data.csv'\n",
    "\n",
    "# Ensure the input file exists\n",
    "assert os.path.isfile(path_preprocessed_data), f'{path_preprocessed_data} not found. Is it a file?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read table with preprocesseded data that will be used in this script\n",
    "track(\"Reading preprocessed data\")\n",
    "preprocessed_data = pd.read_csv(path_preprocessed_data)\n",
    "track(\"Finished reading preprocessed data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the column containing the name of the author\n",
    "data = preprocessed_data.loc[:, preprocessed_data.columns != 'author']\n",
    "track(\"Author column was dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_cols = ['author_timezone', 'commit_message', 'n_commits', 'n_projects_c', 'complexity', 'cognitive_complexity', 'duplicated_blocks', 'duplicated_files', \n",
    "                'duplicated_lines_density', 'open_issues', 'files', 'comment_lines_density', 'n_measures', 'n_projects_m', 'effort', 'message']\n",
    "\n",
    "#quality_cols = ['violations', 'blocker_violations', 'critical_violations', 'major_violations', 'minor_violations', 'n_projects_i']\n",
    "# info_violations, 'sqale_debt_ratio', 'code_smells', 'bugs', 'reliability_rating', 'vulnerabilities', 'security_rating', \n",
    "#'blocker','critical', 'info', 'major', 'minor', 'issue_code_length', 'n_issues'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means with normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Starting mix max scaling of data\")\n",
    "# Min max scaling of data\n",
    "track(\"Finished mix max scaling of data\")\n",
    "min_max_data = transform_dataset(data[cluster_cols],type=\"min_max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of clusters and which cluster is every author\n",
    "track(\"Starting WCSS and Elbow method for choosing the number of clusters\")\n",
    "clusters_none , number_of_clusters_none , silhouette_none = define_num_clusters(min_max_data,min_k=4, max_k=6, method=\"Normalized_data\")\n",
    "track(\"Finished WCSS and Elbow method for choosing the number of clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means with PCA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCA\n",
    "track(\"Starting to compute PCA\")\n",
    "PCA_data = compute_PCA(min_max_data, min_var=0.95)\n",
    "track(\"Finished computing PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of clusters and which cluster is every author\n",
    "track(\"Starting WCSS and Elbow method for choosing the number of clusters\")\n",
    "clusters_PCA, number_of_clusters_PCA, silhouette_PCA = define_num_clusters(PCA_data,min_k=4,max_k=6,method =\"PCA\")\n",
    "track(\"Finished WCSS and Elbow method for choosing the number of clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means with UMAP data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Starting standardization of data\")\n",
    "# Standardization of data\n",
    "track(\"Finished standardization of data\")\n",
    "standardized_data = transform_dataset(data[cluster_cols],type=\"standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute UMAP\n",
    "track(\"Starting to compute UMAP\")\n",
    "UMAP_data = compute_UMAP(standardized_data,n_neighbors=30,min_dist=0.01,n_components=10)\n",
    "track(\"Finished computing UMAP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the number of clusters and which cluster is every author\n",
    "track(\"Starting WCSS and Elbow method for choosing the number of clusters\")\n",
    "clusters_UMAP, number_of_clusters_UMAP, silhouette_UMAP = define_num_clusters(UMAP_data,min_k=4,max_k=6,method ='UMAP')\n",
    "track(\"Finished WCSS and Elbow method for choosing the number of clusters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if silhouette_PCA > silhouette_UMAP and silhouette_PCA > silhouette_none:\n",
    "    track(\"Computing PCA plot with K-Means clusters\")\n",
    "    plot_clusters(clusters_PCA, min_max_data,standardized_data,\"PCA\")\n",
    "    preprocessed_data['clusters'] = clusters_PCA\n",
    "elif silhouette_PCA < silhouette_UMAP and silhouette_UMAP > silhouette_none:\n",
    "    track(\"Computing UMAP plot with K-Means clusters\")\n",
    "    plot_clusters(clusters_UMAP, min_max_data, standardized_data,\"UMAP\")\n",
    "    preprocessed_data['clusters'] = clusters_UMAP\n",
    "else:\n",
    "    track(\"Computing PCA and UMAP plots with K-Means clusters\")\n",
    "    plot_clusters(clusters_none, min_max_data, standardized_data,\"None\")\n",
    "    preprocessed_data['clusters'] = clusters_none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Metric calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Creating quality rating\")\n",
    "quality_rating_data = preprocessed_data.groupby('clusters').agg({\n",
    "                            'violations': 'mean',\n",
    "                            'blocker_violations': 'mean',\n",
    "                            'critical_violations': 'mean',\n",
    "                            'major_violations': 'mean',\n",
    "                            'minor_violations': 'mean',\n",
    "                            'blocker': 'mean',\n",
    "                            'critical': 'mean',\n",
    "                            'major': 'mean',\n",
    "                            'minor': 'mean',\n",
    "                            'code_smells': 'mean',\n",
    "                            'bugs': 'mean',\n",
    "                            'vulnerabilities': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate ponderated mean of the violations variables\n",
    "violations = quality_rating_data[[\"blocker_violations\", \"critical_violations\", \"major_violations\", \"minor_violations\"]] \n",
    "violations = violations*[0.5, 0.4, 0.07, 0.03]\n",
    "violations = np.sum(violations, axis=1)\n",
    "# Calculate ponderated mean of the severity issues variables\n",
    "severity = quality_rating_data[[\"blocker\", \"critical\", \"major\", \"minor\"]]\n",
    "severity = severity*[0.5, 0.4, 0.15, 0.05]\n",
    "severity = np.sum(severity, axis=1)\n",
    "# Add violations and severity columns in the dataset\n",
    "quality_rating_data['violations'] = violations\n",
    "quality_rating_data['severity'] = severity\n",
    "# Discard all types of violations and severity issues columns\n",
    "quality_rating_data = quality_rating_data.drop([\"clusters\", \"blocker_violations\", \"critical_violations\", \"major_violations\", \"minor_violations\", \"blocker\", \"critical\", \"major\", \"minor\"], axis=1)\n",
    "quality_rating_data\n",
    "# compute the mean of each cluster\n",
    "quality_rating_data = quality_rating_data[[\"violations\", \"code_smells\",\t\"bugs\",\t\"vulnerabilities\",\t\"severity\"]]\n",
    "quality_rating_data\n",
    "suma = np.sum(quality_rating_data, axis=1)\n",
    "\n",
    "total = np.sum(suma)\n",
    "quality_rating = 1 - (suma/total)\n",
    "track(\"Finished creating quality rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.968450\n",
       "1    0.176544\n",
       "2    0.956348\n",
       "3    0.898658\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving quality metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the quality metric to the cluster data\n",
    "dic_quality = {}\n",
    "for i in range(len(quality_rating)):\n",
    "    dic_quality[i] = quality_rating[i]\n",
    "\n",
    "preprocessed_data['quality_rating'] = preprocessed_data['clusters'].map(dic_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.968450\n",
       "1    0.176544\n",
       "2    0.956348\n",
       "3    0.898658\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quality_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assigning name to the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# Assigning name to the clusters, depending on the quality rating\n",
    "dic_name_clusters = {}\n",
    "\n",
    "for i in range(len(quality_rating)):\n",
    "    if quality_rating[i] <= 0.2:\n",
    "        dic_name_clusters[i] = 'Extremely bad programming skills'\n",
    "    elif quality_rating[i] <= 0.4 and quality_rating[i] > 0.2:\n",
    "        dic_name_clusters[i] = 'Poor programming skills'\n",
    "    elif quality_rating[i] <= 0.6 and quality_rating[i] > 0.4:\n",
    "        dic_name_clusters[i] = 'Bad programming skills' \n",
    "    elif quality_rating[i] <= 0.8 and quality_rating[i] > 0.6:\n",
    "        dic_name_clusters[i] = 'Average programming skills'\n",
    "    elif quality_rating[i] <= 0.9 and quality_rating[i] > 0.8:\n",
    "        dic_name_clusters[i] = 'Good programming skills'\n",
    "    else:\n",
    "        dic_name_clusters[i] = 'Excellent programming skills'   \n",
    "\n",
    "preprocessed_data['clusters'] = preprocessed_data['clusters'].map(dic_name_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Excellent programming skills',\n",
       " 1: 'Extremely bad programming skills',\n",
       " 2: 'Excellent programming skills',\n",
       " 3: 'Good programming skills'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_name_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving data with clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, the final dataframe with the cluster variable and the quality rating is written in the suitable folder.\n",
    "\n",
    "try: os.mkdir(\"../../temp_data/\")\n",
    "except: pass\n",
    "preprocessed_data.to_csv(\"../../temp_data/model_data_with_clusters.csv\", index_label = \"author\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6cd1d83718134f315460238d88b359275c9d4f8cd456942e203aa190d4cb480b"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
