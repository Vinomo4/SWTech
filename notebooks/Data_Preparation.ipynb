{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Data preparation of the TechDebt dataset. Concretely, from the following tables:\n",
    "- GIT_COMMITS\n",
    "- GIT_COMMITS_CHANGES\n",
    "- JIRA_ISSUES\n",
    "- SONAR_ANALYSIS\n",
    "- SONAR_ISSUES\n",
    "- SONAR_MEASURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and packages\n",
    "# Miscellaneous libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import collections\n",
    "from prettytable import PrettyTable\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the path of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path of the data files\n",
    "path = '../data/raw/'\n",
    "path_git_commits = path + 'GIT_COMMITS.csv'\n",
    "path_git_commits_changes = path + 'GIT_COMMITS_CHANGES.csv'\n",
    "path_jira_issues = path + 'JIRA_ISSUES.csv'\n",
    "path_sonar_analysis = path + 'SONAR_ANALYSIS.csv'\n",
    "path_sonar_issues = path + 'SONAR_ISSUES.csv'\n",
    "path_sonar_measures = path + 'SONAR_MEASURES.csv'\n",
    "\n",
    "# Ensure the input file exist\n",
    "assert os.path.isfile(path_git_commits), f'{path_git_commits} not found. Is it a file?'\n",
    "assert os.path.isfile(path_git_commits_changes), f'{path_git_commits_changes} not found. Is it a file?'\n",
    "assert os.path.isfile(path_jira_issues), f'{path_jira_issues} not found. Is it a file?'\n",
    "assert os.path.isfile(path_sonar_analysis), f'{path_sonar_analysis} not found. Is it a file?'\n",
    "assert os.path.isfile(path_sonar_issues), f'{path_sonar_issues} not found. Is it a file?'\n",
    "assert os.path.isfile(path_sonar_measures), f'{path_sonar_measures} not found. Is it a file?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files\n",
    "git_commits_changes = spark.read.csv(path_git_commits_changes,header=True).toPandas()\n",
    "git_commits = pd.read_csv(path_git_commits)\n",
    "jira_issues = pd.read_csv(path_jira_issues)\n",
    "sonar_analysis = pd.read_csv(path_sonar_analysis)\n",
    "sonar_issues = pd.read_csv(path_sonar_issues)\n",
    "sonar_measures = pd.read_csv(path_sonar_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define selected variables\n",
    "In the following section we are only selecting the useful variables for the project. The election process has been studied previusly, in the Data Understanding step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables of interest for each dataframe\n",
    "git_commits_changes_names = ['COMMIT_HASH','DATE','LINES_ADDED','LINES_REMOVED']\n",
    "git_commits_names = ['PROJECT_ID','COMMIT_HASH','AUTHOR','AUTHOR_DATE','AUTHOR_TIMEZONE','COMMIT_MESSAGE']\n",
    "jira_issues_names = ['HASH']\n",
    "sonar_analysis_names = ['PROJECT_ID','ANALYSIS_KEY','REVISION']\n",
    "sonar_issues_names = ['CREATION_ANALYSIS_KEY','SEVERITY','STATUS','EFFORT','MESSAGE','START_LINE','END_LINE','CLOSE_ANALYSIS_KEY']\n",
    "sonar_measures_names = ['analysis_key','complexity' ,'cognitive_complexity', 'coverage', 'duplicated_blocks', 'duplicated_files', \n",
    "                        'duplicated_lines_density', 'violations','blocker_violations','critical_violations','major_violations','minor_violations','info_violations','false_positive_issues','open_issues','reopened_issues','confirmed_issues', 'sqale_debt_ratio','code_smells','bugs','reliability_rating','vulnerabilities','security_rating','files', 'comment_lines_density']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables of interest\n",
    "git_commits_changes = git_commits_changes[git_commits_changes_names]\n",
    "git_commits = git_commits[git_commits_names]\n",
    "jira_issues = jira_issues[jira_issues_names]\n",
    "sonar_analysis = sonar_analysis[sonar_analysis_names]\n",
    "sonar_issues = sonar_issues[sonar_issues_names]\n",
    "sonar_measures = sonar_measures[sonar_measures_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "dtypes = ['uint8','int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_na(name_tables, dataframes, dtypes):\n",
    "    '''\n",
    "    Objective:\n",
    "        - Delete all NA's from the dataframe passed\n",
    "    Input:\n",
    "        - Name_tables : String of the names of the tables\n",
    "        - Dataframe : String of the tables and their selected columns\n",
    "        - Numerical : Numerical types\n",
    "        \n",
    "    Output: \n",
    "        - Table with the dataset sizes before and after deleting all NA:\n",
    "        [\"Table name\", \"Total number of rows\", \"Number of NA\", \"Total number of rows after deleting NA\"]\n",
    "    '''\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Table name\", \"Total number of rows\", \"Number of NA\", \"Total number of rows after deleting NA\"]\n",
    "    for i in range(len(name_tables)):\n",
    "        dataframe_numerical = dataframes[i].select_dtypes(include=dtypes)\n",
    "        total_rows = dataframe_numerical.shape[0]\n",
    "        # Rows with na's\n",
    "        rows_with_NA = sum([True for idx,row in dataframe_numerical.iterrows() if any(row.isnull())])\n",
    "        # Delete rows that contain na's\n",
    "        dataframe_numerical = dataframe_numerical.dropna()\n",
    "        total_rows_without_NA = dataframe_numerical.shape[0]\n",
    "        table.add_row([name_tables[i] , total_rows, rows_with_NA,total_rows_without_NA])\n",
    "        dataframes[i] = dataframe_numerical\n",
    "    print(table)\n",
    "    return dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_categorical_variables(table_names, variable_names, dataframes):\n",
    "    '''\n",
    "    Objective:\n",
    "        - Analyse the categorical variables to be encoded\n",
    "    Input:\n",
    "        - Table_names : String of the names of the tables\n",
    "        - Variable_names : String of the categorical variables corresponding to the table\n",
    "        - Dataframes : String of the tables and their selected columns\n",
    "        \n",
    "    Output: \n",
    "        - Table with the categorical variables levels\n",
    "        [\"Table name\", \"Variable name\", \"Number of levels\", \"Types\"]\n",
    "    '''\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Table name\", \"Variable name\", \"Number of levels\", \"Types\"]\n",
    "    for i in range(len(table_names)):\n",
    "        for j in range(len(variable_names)):\n",
    "            table.add_row([table_names[i], variable_names[j], len(dataframes[i][variable_names[j]].unique()), dataframes[i][variable_names[j]].unique()])\n",
    "    print(table)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(table, variable):\n",
    "    '''\n",
    "    Objective:\n",
    "        - Encode the categorical variable passed from the table to one-hot encoding\n",
    "    Input:\n",
    "        - Table : String of the table name\n",
    "        - Variable : String of the categorical variable corresponding to the table \n",
    "        \n",
    "    Output: \n",
    "        - Table with the categorical variable encoded to one-hot \n",
    "        [\"Table name\", \"Variable name\", \"Number of levels\", \"Types\"]\n",
    "    '''\n",
    "    variable_dummies = pd.get_dummies(table[variable])\n",
    "    table = table.drop(variable, axis=1)\n",
    "    table = table.join(variable_dummies)\n",
    "    return table\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_length(table,column):\n",
    "    '''\n",
    "    Objective:\n",
    "        - Generate a column containgthe length of different messages and delete the column\n",
    "          that contains the orignal text.\n",
    "    Input:\n",
    "        - table : Dataframe that wants to be used.\n",
    "        - column : Name of the variable that contains the messages.\n",
    "    Output:\n",
    "        - None\n",
    "    '''\n",
    "    message_length = []\n",
    "    for msg in table[column]:\n",
    "        message_length.append(len(msg))\n",
    "    # Reassign the MESSAGE variable to its length instead of the initial string \\n\",\n",
    "    table[column] = message_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NA values\n",
    "Deleting all NA values from the tables by using the global function implemented above delete_na()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = [\"SONAR_MEASURES\", \"SONAR_ISSUES\", \"SONAR_ANALYSIS\", \"JIRA_ISSUES\",\"GIT_COMMITS\",\"GIT_COMMITS_CHANGES\"]\n",
    "tables = [sonar_measures, sonar_issues, sonar_analysis, jira_issues,git_commits, git_commits_changes]\n",
    "[sonar_measures, sonar_issues, sonar_analysis, jira_issues,git_commits, git_commits_changes] = delete_na(table_names, tables, dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, in the GIT_COMMITS table, we also find rows that contain the value \"No Author\" in the AUTHOR column.\n",
    "As we cannot know if all those commits come from an unique unidentified person or from multiple ones, we decided to eliminate such rows, as seen in the Data Quality task, they represent a minor percentage of the table length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = PrettyTable()\n",
    "table.field_names = [\"Table name\",\"Total number of rows\", \"Number of missing authors\",\"Number of rows after deleting missing authors\"]\n",
    "total_rows = git_commits.shape[0]\n",
    "# Rows with Missing authors\n",
    "rows_with_MA = sum(git_commits.AUTHOR == \"No Author\")\n",
    "# Delete rows that contain missing authors and reseting the DF index.\",\n",
    "git_commits = git_commits.drop(git_commits[git_commits.AUTHOR == \"No Author\"].index)\n",
    "git_commits.reset_index(drop= True)\n",
    "total_rows_without_MA = git_commits.shape[0]\n",
    "table.add_row([\"GIT_COMMITS\" , total_rows, rows_with_MA,total_rows_without_MA])\n",
    "print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Values\n",
    "The next step is to analyse the categorical variables and encoding them.\n",
    "For the SONAR_MEASURES, JIRA_ISSUES, SONAR_ANALYSIS table (add more if necessary) there are not categorical varibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = [\"SONAR_ISSUES\"]\n",
    "variable_names = [\"SEVERITY\", \"STATUS\", \"EFFORT\"]\n",
    "dataframes = [sonar_issues]\n",
    "analyse_categorical_variables(table_names, variable_names, dataframes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the chunk above, the SEVERITY and STATUS variables have 5 and 1 levels respectively. In our case, we have performed the One-hot encoding for the SEVERITY variable. For the STATUS variable, efore deleting all NA, there was the OPENED level. However, all rows with an OPENED status contained NA, which means that for this variable we only have the CLOSED level. \n",
    "When joining the tables, we will calulate the mean of each types each author has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_issues = one_hot_encoding(sonar_issues, \"SEVERITY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_issues = one_hot_encoding(sonar_issues, \"STATUS\")\n",
    "sonar_issues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MESSAGE and COMMIT_MESSAGE variables\n",
    "In the following section, we will encode the MESSAGE and COMMIT_MESSAGE variables for the SONAR_ISSUES table and GIT_COMMITS table respectively. For those variables, we will calulate the length of the message for each issue/commit, and reassigning the column with that new value instead of the text from the original message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_length(sonar_issues,\"MESSAGE\")\n",
    "message_length(git_commits,\"COMMIT_MESSAGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUMBER_OF_LINES variable\n",
    "\n",
    "In the following cells we will proceed to computate the length mean per issue with the START_LINE and END_LINE variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_length = []\n",
    "for index, row in sonar_issues.iterrows():\n",
    "    diff = row['END_LINE'] - row['START_LINE']\n",
    "    issue_length.append(diff)\n",
    "len(issue_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = set(issue_length)\n",
    "len(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_issues = sonar_issues.drop('START_LINE', axis=1)\n",
    "sonar_issues = sonar_issues.drop('END_LINE', axis=1)\n",
    "sonar_issues['ISSUE_CODE_LENGTH'] = issue_length\n",
    "sonar_issues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resum:\n",
    "- 66711 rows in sonar analysis\n",
    "- 719186 rows un cop fent el join amb la variable creation_Analysis key\n",
    "- 728409 rows un cop fent el join amb el CLOSE_ANALYSIS_KEY\n",
    "- 1447595 rows dels dos joints \n",
    "- 1200310 unique rows en total dels dos joints\n",
    "- 1197656 rows un cop fent el join del SONRA_MEASURES i SONAR_COMPLETE (sonar_analysis + sonar_issues)\n",
    "- 1083655 unique rows en total dels dos joints\n",
    "\n",
    "(ho he calulat abans amb datafrme.shape[0] per√≤ els chuncks els he eliminat per netejar el codi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining SONAR_ANALYSIS with SONAR_ISSUES\n",
    "sonar_complete_1 = pd.merge(sonar_issues, sonar_analysis, left_on='CREATION_ANALYSIS_KEY', right_on='ANALYSIS_KEY', how='inner')\n",
    "sonar_complete_2 = pd.merge(sonar_issues, sonar_analysis, left_on='CLOSE_ANALYSIS_KEY', right_on='ANALYSIS_KEY', how='inner')\n",
    "sonar_complete_1 = sonar_complete_1.drop('CREATION_ANALYSIS_KEY', axis=1)\n",
    "sonar_complete_1 = sonar_complete_1.drop('CLOSE_ANALYSIS_KEY', axis=1)\n",
    "sonar_complete_2 = sonar_complete_2.drop('CREATION_ANALYSIS_KEY', axis=1)\n",
    "sonar_complete_2 = sonar_complete_2.drop('CLOSE_ANALYSIS_KEY', axis=1)\n",
    "\n",
    "sonar_complete = pd.concat([sonar_complete_1, sonar_complete_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_complete.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting duplicated rows\n",
    "sonar_complete = sonar_complete[sonar_complete.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_complete.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining SONAR_ANALYSIS with SONAR_MEASURES\n",
    "sonar_complete = pd.merge(sonar_complete, sonar_measures, left_on='ANALYSIS_KEY', right_on='analysis_key', how='inner')\n",
    "sonar_complete = sonar_complete.drop('ANALYSIS_KEY', axis=1)\n",
    "sonar_complete.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_complete.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting duplicated rows\n",
    "sonar_complete = sonar_complete[sonar_complete.duplicated()]\n",
    "sonar_complete.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = sonar_complete['analysis_key'].unique()\n",
    "len(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_complete =  pd.merge(git_commits, git_commits_changes, left_on='COMMIT_HASH', right_on='COMMIT_HASH', how='inner')\n",
    "git_complete.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_table = pd.merge(git_complete, sonar_complete, left_on='COMMIT_HASH', right_on='REVISION', how='inner')\n",
    "final_table.shape[0]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
