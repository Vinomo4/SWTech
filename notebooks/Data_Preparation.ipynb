{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Data preparation of the TechDebt dataset. Concretely, from the following tables:\n",
    "- GIT_COMMITS\n",
    "- GIT_COMMITS_CHANGES\n",
    "- JIRA_ISSUES\n",
    "- SONAR_ANALYSIS\n",
    "- SONAR_ISSUES\n",
    "- SONAR_MEASURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and packages\n",
    "# Miscellaneous libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import collections\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "os.getcwd()\n",
    "os.chdir( '../src/features')\n",
    "from tracking import track\n",
    "from preparation_data import delete_na, analyse_categorical_variables, one_hot_encoding, message_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the path of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Defining path of the data files\")\n",
    "# Define the path of the data files\n",
    "path1 = '../../data/raw/'\n",
    "path2 = \"../data/raw/\"\n",
    "path_git_commits = path1 + 'GIT_COMMITS.csv'\n",
    "path_git_commits_changes = path2 + 'GIT_COMMITS_CHANGES.csv'\n",
    "path_jira_issues = path1 + 'JIRA_ISSUES.csv'\n",
    "path_sonar_analysis = path1 + 'SONAR_ANALYSIS.csv'\n",
    "path_sonar_issues = path1 + 'SONAR_ISSUES.csv'\n",
    "path_sonar_measures = path1 + 'SONAR_MEASURES.csv'\n",
    "\n",
    "# Ensure the input file exist\n",
    "assert os.path.isfile(path_git_commits), f'{path_git_commits} not found. Is it a file?'\n",
    "assert os.path.isfile(\"../\"+path_git_commits_changes), f'{path_git_commits_changes} not found. Is it a file?'\n",
    "assert os.path.isfile(path_jira_issues), f'{path_jira_issues} not found. Is it a file?'\n",
    "assert os.path.isfile(path_sonar_analysis), f'{path_sonar_analysis} not found. Is it a file?'\n",
    "assert os.path.isfile(path_sonar_issues), f'{path_sonar_issues} not found. Is it a file?'\n",
    "assert os.path.isfile(path_sonar_measures), f'{path_sonar_measures} not found. Is it a file?'\n",
    "track(\"Finishing defining path of the data files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Reading files\")\n",
    "# Read the files\n",
    "git_commits_changes = spark.read.csv(path_git_commits_changes,header=True).toPandas()\n",
    "git_commits = pd.read_csv(path_git_commits)\n",
    "jira_issues = pd.read_csv(path_jira_issues)\n",
    "sonar_analysis = pd.read_csv(path_sonar_analysis)\n",
    "sonar_issues = pd.read_csv(path_sonar_issues)\n",
    "sonar_measures = pd.read_csv(path_sonar_measures)\n",
    "track(\"Finishing reading files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define selected variables\n",
    "In the following section we are only selecting the useful variables for the project. The election process has been studied previusly, in the Data Understanding step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables of interest for each dataframe\n",
    "git_commits_changes_names = ['COMMIT_HASH','DATE','LINES_ADDED','LINES_REMOVED']\n",
    "git_commits_names = ['PROJECT_ID','COMMIT_HASH','AUTHOR','AUTHOR_DATE','AUTHOR_TIMEZONE','COMMIT_MESSAGE']\n",
    "jira_issues_names = ['HASH']\n",
    "sonar_analysis_names = ['PROJECT_ID','ANALYSIS_KEY','REVISION']\n",
    "sonar_issues_names = ['CREATION_ANALYSIS_KEY','SEVERITY','STATUS','EFFORT','MESSAGE','START_LINE','END_LINE','CLOSE_ANALYSIS_KEY']\n",
    "sonar_measures_names = ['analysis_key','complexity' ,'cognitive_complexity', 'coverage', 'duplicated_blocks', 'duplicated_files', \n",
    "                        'duplicated_lines_density', 'violations','blocker_violations','critical_violations','major_violations','minor_violations','info_violations','false_positive_issues','open_issues','reopened_issues','confirmed_issues', 'sqale_debt_ratio','code_smells','bugs','reliability_rating','vulnerabilities','security_rating','files', 'comment_lines_density']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables of interest\n",
    "git_commits_changes = git_commits_changes[git_commits_changes_names]\n",
    "git_commits = git_commits[git_commits_names]\n",
    "jira_issues = jira_issues[jira_issues_names]\n",
    "sonar_analysis = sonar_analysis[sonar_analysis_names]\n",
    "sonar_issues = sonar_issues[sonar_issues_names]\n",
    "sonar_measures = sonar_measures[sonar_measures_names]\n",
    "track(\"Finishing selecting variables of interest for each dataframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Starting defining numercial types\")\n",
    "# Select columns of interest\n",
    "dtypes = ['uint8','int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'object']\n",
    "track(\"Finishing defining numercial types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NA values\n",
    "Deleting all NA values from the tables by using the global function implemented above delete_na()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Starting analysing NA values from all tables\")\n",
    "tables = [sonar_measures, sonar_issues, sonar_analysis, jira_issues,git_commits, git_commits_changes]\n",
    "[sonar_measures, sonar_issues, sonar_analysis, jira_issues,git_commits, git_commits_changes] = delete_na(tables, dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, in the GIT_COMMITS table, we also find rows that contain the value \"No Author\" in the AUTHOR column.\n",
    "As we cannot know if all those commits come from an unique unidentified person or from multiple ones, we decided to eliminate such rows, as seen in the Data Quality task, they represent a minor percentage of the table length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete rows that contain missing authors and reseting the DF index.\",\n",
    "git_commits = git_commits.drop(git_commits[git_commits.AUTHOR == \"No Author\"].index)\n",
    "git_commits.reset_index(drop= True)\n",
    "track(\"Finishing analysing NA values from all tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Values\n",
    "The next step is to analyse the categorical variables and encoding them.\n",
    "For the SONAR_MEASURES, JIRA_ISSUES, SONAR_ANALYSIS table (add more if necessary) there are not categorical varibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+------------------+-----------------------------------------------+\n",
      "|  Table name  | Variable name | Number of levels |                     Types                     |\n",
      "+--------------+---------------+------------------+-----------------------------------------------+\n",
      "| SONAR_ISSUES |    SEVERITY   |        5         | ['INFO' 'MINOR' 'MAJOR' 'CRITICAL' 'BLOCKER'] |\n",
      "| SONAR_ISSUES |     STATUS    |        1         |                   ['CLOSED']                  |\n",
      "+--------------+---------------+------------------+-----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "track(\"Starting analysing categorical variables\")\n",
    "table_names = [\"SONAR_ISSUES\"]\n",
    "variable_names = [\"SEVERITY\", \"STATUS\"]\n",
    "dataframes = [sonar_issues]\n",
    "analyse_categorical_variables(table_names, variable_names, dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the chunk above, the SEVERITY and STATUS variables have 5 and 1 levels respectively. In our case, we have performed the One-hot encoding for the SEVERITY variable. For the STATUS variable, efore deleting all NA, there was the OPENED level. However, all rows with an OPENED status contained NA, which means that for this variable we only have the CLOSED level. \n",
    "When joining the tables, we will calulate the mean of each types each author has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_issues = one_hot_encoding(sonar_issues, \"SEVERITY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_issues = one_hot_encoding(sonar_issues, \"STATUS\")\n",
    "track(\"Finishing analysing categorical variables from all tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MESSAGE and COMMIT_MESSAGE variables\n",
    "In the following section, we will encode the MESSAGE and COMMIT_MESSAGE variables for the SONAR_ISSUES table and GIT_COMMITS table respectively. For those variables, we will calulate the length of the message for each issue/commit, and reassigning the column with that new value instead of the text from the original message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Starting codifying MESSAGE and COMMIT_MESSAGE variables using message_length() function\")\n",
    "message_length(sonar_issues,\"MESSAGE\")\n",
    "message_length(git_commits,\"COMMIT_MESSAGE\")\n",
    "track(\"Finishing codifying MESSAGE and COMMIT_MESSAGE variables using message_length() function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISSUE_CODE_LENGTH variable\n",
    "\n",
    "In the following cells we will proceed to computate the length mean per issue with the START_LINE and END_LINE variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Starting creating ISSUE_CODE_LENGTH variable for SONAR_ISSUES table\")\n",
    "issue_length = []\n",
    "for index, row in sonar_issues.iterrows():\n",
    "    diff = row['END_LINE'] - row['START_LINE']\n",
    "    issue_length.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_issues = sonar_issues.drop('START_LINE', axis=1)\n",
    "sonar_issues = sonar_issues.drop('END_LINE', axis=1)\n",
    "sonar_issues['ISSUE_CODE_LENGTH'] = issue_length\n",
    "sonar_issues.head()\n",
    "track(\"Starting creating ISSUE_CODE_LENGTH variable for SONAR_ISSUES table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Starting joining SONAR tables\")\n",
    "# Joining SONAR_ANALYSIS with SONAR_ISSUES\n",
    "sonar_complete_1 = pd.merge(sonar_issues, sonar_analysis, left_on='CREATION_ANALYSIS_KEY', right_on='ANALYSIS_KEY', how='inner')\n",
    "sonar_complete_2 = pd.merge(sonar_issues, sonar_analysis, left_on='CLOSE_ANALYSIS_KEY', right_on='ANALYSIS_KEY', how='inner')\n",
    "sonar_complete_1 = sonar_complete_1.drop('CREATION_ANALYSIS_KEY', axis=1)\n",
    "sonar_complete_1 = sonar_complete_1.drop('CLOSE_ANALYSIS_KEY', axis=1)\n",
    "sonar_complete_2 = sonar_complete_2.drop('CREATION_ANALYSIS_KEY', axis=1)\n",
    "sonar_complete_2 = sonar_complete_2.drop('CLOSE_ANALYSIS_KEY', axis=1)\n",
    "\n",
    "sonar_complete = pd.concat([sonar_complete_1, sonar_complete_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting duplicated rows\n",
    "sonar_complete = sonar_complete.drop_duplicates()\n",
    "sonar_complete = sonar_complete.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining SONAR_ANALYSIS with SONAR_MEASURES\n",
    "sonar_complete = pd.merge(sonar_complete, sonar_measures, left_on='ANALYSIS_KEY', right_on='analysis_key', how='inner')\n",
    "sonar_complete = sonar_complete.drop('ANALYSIS_KEY', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting duplicated rows\n",
    "sonar_complete = sonar_complete.drop_duplicates()\n",
    "sonar_complete = sonar_complete.reset_index(drop = True)\n",
    "track(\"Finishing joining SONAR tables\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "track(\"Starting joining COMMIT tables\")\n",
    "git_complete =  pd.merge(git_commits, git_commits_changes, left_on='COMMIT_HASH', right_on='COMMIT_HASH', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to execute the groupby functions, first we need to associate each commit to\n",
    "# an author in the sonar_complete table. To do so, a dictionary of commit- author will be created.\n",
    "\n",
    "commit_author_dict = {}\n",
    "for i in range(len(git_complete)):\n",
    "    commit_author_dict[git_complete[\"COMMIT_HASH\"][i]] = git_complete[\"AUTHOR\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once is complete, we will substitute the REVISON variable of the sonar_complete table \n",
    "# for the respective author.\n",
    "\n",
    "# First, we obtain the values.\n",
    "\n",
    "authors_measures = []\n",
    "\n",
    "for i in range(len(sonar_complete)):\n",
    "    aux = sonar_complete[\"REVISION\"][i]\n",
    "    if commit_author_dict.get(aux): authors_measures.append(commit_author_dict[aux])\n",
    "    else: authors_measures.append(np.nan)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
