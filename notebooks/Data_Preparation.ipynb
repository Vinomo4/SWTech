{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Data preparation of the TechDebt dataset. Concretely, from the following tables:\n",
    "- GIT_COMMITS\n",
    "- GIT_COMMITS_CHANGES\n",
    "- JIRA_ISSUES\n",
    "- SONAR_ANALYSIS\n",
    "- SONAR_ISSUES\n",
    "- SONAR_MEASURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and packages\n",
    "# Miscellaneous libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import collections\n",
    "from prettytable import PrettyTable\n",
    "import dask.dataframe as dd\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the path of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path of the data files\n",
    "path = '../data/raw/'\n",
    "path_git_commits = path + 'GIT_COMMITS.csv'\n",
    "path_git_commits_changes = path + 'GIT_COMMITS_CHANGES.csv'\n",
    "path_jira_issues = path + 'JIRA_ISSUES.csv'\n",
    "path_sonar_analysis = path + 'SONAR_ANALYSIS.csv'\n",
    "path_sonar_issues = path + 'SONAR_ISSUES.csv'\n",
    "path_sonar_measures = path + 'SONAR_MEASURES.csv'\n",
    "\n",
    "# Ensure the input file exist\n",
    "assert os.path.isfile(path_git_commits), f'{path_git_commits} not found. Is it a file?'\n",
    "assert os.path.isfile(path_git_commits_changes), f'{path_git_commits_changes} not found. Is it a file?'\n",
    "assert os.path.isfile(path_jira_issues), f'{path_jira_issues} not found. Is it a file?'\n",
    "assert os.path.isfile(path_sonar_analysis), f'{path_sonar_analysis} not found. Is it a file?'\n",
    "assert os.path.isfile(path_sonar_issues), f'{path_sonar_issues} not found. Is it a file?'\n",
    "assert os.path.isfile(path_sonar_measures), f'{path_sonar_measures} not found. Is it a file?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files\n",
    "git_commits_changes = spark.read.csv(path_git_commits_changes,header=True).toPandas()\n",
    "git_commits = pd.read_csv(path_git_commits)\n",
    "jira_issues = pd.read_csv(path_jira_issues)\n",
    "sonar_analysis = pd.read_csv(path_sonar_analysis)\n",
    "sonar_issues = pd.read_csv(path_sonar_issues)\n",
    "sonar_measures = pd.read_csv(path_sonar_measures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define selected variables\n",
    "In the following section we are only selecting the useful variables for the project. The election process has been studied previusly, in the Data Understanding step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables of interest for each dataframe\n",
    "git_commits_changes_names = ['COMMIT_HASH','DATE','LINES_ADDED','LINES_REMOVED']\n",
    "git_commits_names = ['PROJECT_ID','COMMIT_HASH','AUTHOR','AUTHOR_DATE','AUTHOR_TIMEZONE','COMMIT_MESSAGE']\n",
    "jira_issues_names = ['HASH']\n",
    "sonar_analysis_names = ['PROJECT_ID','ANALYSIS_KEY','REVISION']\n",
    "sonar_issues_names = ['CREATION_ANALYSIS_KEY','SEVERITY','STATUS','EFFORT','MESSAGE','START_LINE','END_LINE','CLOSE_ANALYSIS_KEY']\n",
    "sonar_measures_names = ['analysis_key','complexity' ,'cognitive_complexity', 'coverage', 'duplicated_blocks', 'duplicated_files', \n",
    "                        'duplicated_lines_density', 'violations','blocker_violations','critical_violations','major_violations','minor_violations','info_violations','false_positive_issues','open_issues','reopened_issues','confirmed_issues', 'sqale_debt_ratio','code_smells','bugs','reliability_rating','vulnerabilities','security_rating','files', 'comment_lines_density']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select variables of interest\n",
    "git_commits_changes = git_commits_changes[git_commits_changes_names]\n",
    "git_commits = git_commits[git_commits_names]\n",
    "jira_issues = jira_issues[jira_issues_names]\n",
    "sonar_analysis = sonar_analysis[sonar_analysis_names]\n",
    "sonar_issues = sonar_issues[sonar_issues_names]\n",
    "sonar_measures = sonar_measures[sonar_measures_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select columns of interest\n",
    "dtypes = ['uint8','int16', 'int32', 'int64', 'float16', 'float32', 'float64', 'object']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_na(dataframes, dtypes):\n",
    "    '''\n",
    "    Objective:\n",
    "        - Delete all NA's from the dataframe passed\n",
    "    Input:\n",
    "        - Dataframe : String of the tables and their selected columns\n",
    "        - Numerical : Numerical types\n",
    "        \n",
    "    Output: \n",
    "        - Dataframe with the deleted values.\n",
    "    '''\n",
    "    for i in range(len(dataframes)):\n",
    "        dataframe_numerical = dataframes[i].select_dtypes(include=dtypes)\n",
    "        total_rows = dataframe_numerical.shape[0]\n",
    "        # Delete rows that contain na's\n",
    "        dataframe_numerical = dataframe_numerical.dropna()\n",
    "        dataframes[i] = dataframe_numerical\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_categorical_variables(table_names, variable_names, dataframes):\n",
    "    '''\n",
    "    Objective:\n",
    "        - Analyse the categorical variables to be encoded\n",
    "    Input:\n",
    "        - Table_names : String of the names of the tables\n",
    "        - Variable_names : String of the categorical variables corresponding to the table\n",
    "        - Dataframes : String of the tables and their selected columns\n",
    "        \n",
    "    Output: \n",
    "        - Table with the categorical variables levels\n",
    "        [\"Table name\", \"Variable name\", \"Number of levels\", \"Types\"]\n",
    "    '''\n",
    "    table = PrettyTable()\n",
    "    table.field_names = [\"Table name\", \"Variable name\", \"Number of levels\", \"Types\"]\n",
    "    for i in range(len(table_names)):\n",
    "        for j in range(len(variable_names)):\n",
    "            table.add_row([table_names[i], variable_names[j], len(dataframes[i][variable_names[j]].unique()), dataframes[i][variable_names[j]].unique()])\n",
    "    print(table)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(table, variable):\n",
    "    '''\n",
    "    Objective:\n",
    "        - Encode the categorical variable passed from the table to one-hot encoding.\n",
    "          If the categorical variable only has one level, the column is deleted.\n",
    "    Input:\n",
    "        - Table : String of the table name\n",
    "        - Variable : String of the categorical variable corresponding to the table \n",
    "        \n",
    "    Output:\n",
    "        For categorical variables with more than one level:\n",
    "            - Table with the categorical variable encoded to one-hot \n",
    "            [\"Table name\", \"Variable name\", \"Number of levels\", \"Types\"]\n",
    "        For categorical variables with less than one level:\n",
    "            -String indicating so.\n",
    "    '''\n",
    "    # The column contain more than one level.\n",
    "    if len(table[variable].unique()) > 1:\n",
    "        variable_dummies = pd.get_dummies(table[variable])\n",
    "        table = table.drop(variable, axis=1)\n",
    "        table = table.join(variable_dummies)\n",
    "        return table\n",
    "    # The column contain only one value and will be deleted.\n",
    "    else:\n",
    "        return table.drop(variable, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_length(table,column):\n",
    "    '''\n",
    "    Objective:\n",
    "        - Generate a column containgthe length of different messages and delete the column\n",
    "          that contains the orignal text.\n",
    "    Input:\n",
    "        - table : Dataframe that wants to be used.\n",
    "        - column : Name of the variable that contains the messages.\n",
    "    Output:\n",
    "        - None\n",
    "    '''\n",
    "    message_length = []\n",
    "    for msg in table[column]:\n",
    "        message_length.append(len(msg))\n",
    "    # Reassign the MESSAGE variable to its length instead of the initial string \\n\",\n",
    "    table[column] = message_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NA values\n",
    "Deleting all NA values from the tables by using the global function implemented above delete_na()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [sonar_measures, sonar_issues, sonar_analysis, jira_issues,git_commits, git_commits_changes]\n",
    "[sonar_measures, sonar_issues, sonar_analysis, jira_issues,git_commits, git_commits_changes] = delete_na(tables, dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, in the GIT_COMMITS table, we also find rows that contain the value \"No Author\" in the AUTHOR column.\n",
    "As we cannot know if all those commits come from an unique unidentified person or from multiple ones, we decided to eliminate such rows, as seen in the Data Quality task, they represent a minor percentage of the table length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PROJECT_ID</th>\n",
       "      <th>COMMIT_HASH</th>\n",
       "      <th>AUTHOR</th>\n",
       "      <th>AUTHOR_DATE</th>\n",
       "      <th>AUTHOR_TIMEZONE</th>\n",
       "      <th>COMMIT_MESSAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>org.apache:batik</td>\n",
       "      <td>b1ff4af6abfec32fc710d77795bb20a612a82126</td>\n",
       "      <td>James Duncan Davidson</td>\n",
       "      <td>2000-10-01T07:37:01Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Initial revision\\n\\n\\ngit-svn-id: https://svn....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>org.apache:batik</td>\n",
       "      <td>c8d7a13470987f892f7466d55c10e3cee34de31d</td>\n",
       "      <td>James Duncan Davidson</td>\n",
       "      <td>2000-10-01T07:40:39Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Update\\nPR:\\n\\n\\ngit-svn-id: https://svn.apach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>org.apache:batik</td>\n",
       "      <td>93a16402b48ae1cf70ea4bd030479170749bd10a</td>\n",
       "      <td>James Duncan Davidson</td>\n",
       "      <td>2000-10-01T08:15:04Z</td>\n",
       "      <td>0</td>\n",
       "      <td>Added question line (more a test of list/forwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>org.apache:batik</td>\n",
       "      <td>fcaecb541edc03f36b6ec7a792e97dfeabf26117</td>\n",
       "      <td>Dean Jackson</td>\n",
       "      <td>2000-10-02T13:33:11Z</td>\n",
       "      <td>0</td>\n",
       "      <td>testing commit\\n\\n\\ngit-svn-id: https://svn.ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>org.apache:batik</td>\n",
       "      <td>2ecc354fa4f3209adad11560abad29ca3fb9b95d</td>\n",
       "      <td>Dean Jackson</td>\n",
       "      <td>2000-10-02T13:39:12Z</td>\n",
       "      <td>0</td>\n",
       "      <td>undoing the test commit\\n\\n\\ngit-svn-id: https...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81015</th>\n",
       "      <td>org.apache:thrift</td>\n",
       "      <td>53d9c0c20bd5af65676928b9b7a73dcb2cad3d78</td>\n",
       "      <td>Mark Slee</td>\n",
       "      <td>2007-11-26 21:15:40+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Merging EOFException changes from Ben Maurer  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81016</th>\n",
       "      <td>org.apache:thrift</td>\n",
       "      <td>5ab570558f55d73472fbf6c0e66e6e165093c7d8</td>\n",
       "      <td>Mark Slee</td>\n",
       "      <td>2007-11-27 08:38:16+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Fix writeContainerEnd call being inside loop i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81017</th>\n",
       "      <td>org.apache:thrift</td>\n",
       "      <td>844ac12489600d7647f01ab4f9b99d9e1b81e69e</td>\n",
       "      <td>Mark Slee</td>\n",
       "      <td>2007-11-27 08:38:52+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>TJSONProtocol writing support in Java  Summary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81018</th>\n",
       "      <td>org.apache:thrift</td>\n",
       "      <td>256bdc444866b90bbdccfb5343e9c9ea8c22603c</td>\n",
       "      <td>Mark Slee</td>\n",
       "      <td>2007-11-27 08:42:19+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>IPv6 tweaks for Thrift  Summary: Need to pass ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81019</th>\n",
       "      <td>org.apache:thrift</td>\n",
       "      <td>fe6ed0dff423a405fabd61e4bef3e490506ba2ba</td>\n",
       "      <td>Mark Slee</td>\n",
       "      <td>2007-11-27 21:54:38+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>Clean up the TSerializer  Summary: Nested Tran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>81020 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              PROJECT_ID                               COMMIT_HASH  \\\n",
       "0       org.apache:batik  b1ff4af6abfec32fc710d77795bb20a612a82126   \n",
       "1       org.apache:batik  c8d7a13470987f892f7466d55c10e3cee34de31d   \n",
       "2       org.apache:batik  93a16402b48ae1cf70ea4bd030479170749bd10a   \n",
       "3       org.apache:batik  fcaecb541edc03f36b6ec7a792e97dfeabf26117   \n",
       "4       org.apache:batik  2ecc354fa4f3209adad11560abad29ca3fb9b95d   \n",
       "...                  ...                                       ...   \n",
       "81015  org.apache:thrift  53d9c0c20bd5af65676928b9b7a73dcb2cad3d78   \n",
       "81016  org.apache:thrift  5ab570558f55d73472fbf6c0e66e6e165093c7d8   \n",
       "81017  org.apache:thrift  844ac12489600d7647f01ab4f9b99d9e1b81e69e   \n",
       "81018  org.apache:thrift  256bdc444866b90bbdccfb5343e9c9ea8c22603c   \n",
       "81019  org.apache:thrift  fe6ed0dff423a405fabd61e4bef3e490506ba2ba   \n",
       "\n",
       "                      AUTHOR                AUTHOR_DATE  AUTHOR_TIMEZONE  \\\n",
       "0      James Duncan Davidson       2000-10-01T07:37:01Z                0   \n",
       "1      James Duncan Davidson       2000-10-01T07:40:39Z                0   \n",
       "2      James Duncan Davidson       2000-10-01T08:15:04Z                0   \n",
       "3               Dean Jackson       2000-10-02T13:33:11Z                0   \n",
       "4               Dean Jackson       2000-10-02T13:39:12Z                0   \n",
       "...                      ...                        ...              ...   \n",
       "81015              Mark Slee  2007-11-26 21:15:40+00:00                0   \n",
       "81016              Mark Slee  2007-11-27 08:38:16+00:00                0   \n",
       "81017              Mark Slee  2007-11-27 08:38:52+00:00                0   \n",
       "81018              Mark Slee  2007-11-27 08:42:19+00:00                0   \n",
       "81019              Mark Slee  2007-11-27 21:54:38+00:00                0   \n",
       "\n",
       "                                          COMMIT_MESSAGE  \n",
       "0      Initial revision\\n\\n\\ngit-svn-id: https://svn....  \n",
       "1      Update\\nPR:\\n\\n\\ngit-svn-id: https://svn.apach...  \n",
       "2      Added question line (more a test of list/forwa...  \n",
       "3      testing commit\\n\\n\\ngit-svn-id: https://svn.ap...  \n",
       "4      undoing the test commit\\n\\n\\ngit-svn-id: https...  \n",
       "...                                                  ...  \n",
       "81015  Merging EOFException changes from Ben Maurer  ...  \n",
       "81016  Fix writeContainerEnd call being inside loop i...  \n",
       "81017  TJSONProtocol writing support in Java  Summary...  \n",
       "81018  IPv6 tweaks for Thrift  Summary: Need to pass ...  \n",
       "81019  Clean up the TSerializer  Summary: Nested Tran...  \n",
       "\n",
       "[81020 rows x 6 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete rows that contain missing authors and reseting the DF index.\",\n",
    "git_commits = git_commits.drop(git_commits[git_commits.AUTHOR == \"No Author\"].index)\n",
    "git_commits.reset_index(drop= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Values\n",
    "The next step is to analyse the categorical variables and encoding them.\n",
    "For the SONAR_MEASURES, JIRA_ISSUES, SONAR_ANALYSIS table (add more if necessary) there are not categorical varibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+------------------+-----------------------------------------------+\n",
      "|  Table name  | Variable name | Number of levels |                     Types                     |\n",
      "+--------------+---------------+------------------+-----------------------------------------------+\n",
      "| SONAR_ISSUES |    SEVERITY   |        5         | ['INFO' 'MINOR' 'MAJOR' 'CRITICAL' 'BLOCKER'] |\n",
      "| SONAR_ISSUES |     STATUS    |        1         |                   ['CLOSED']                  |\n",
      "+--------------+---------------+------------------+-----------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "table_names = [\"SONAR_ISSUES\"]\n",
    "variable_names = [\"SEVERITY\", \"STATUS\"]\n",
    "dataframes = [sonar_issues]\n",
    "analyse_categorical_variables(table_names, variable_names, dataframes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen in the chunk above, the SEVERITY and STATUS variables have 5 and 1 levels respectively. In our case, we have performed the One-hot encoding for the SEVERITY variable. For the STATUS variable, efore deleting all NA, there was the OPENED level. However, all rows with an OPENED status contained NA, which means that for this variable we only have the CLOSED level. \n",
    "When joining the tables, we will calulate the mean of each types each author has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_issues = one_hot_encoding(sonar_issues, \"SEVERITY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonar_issues = one_hot_encoding(sonar_issues, \"STATUS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MESSAGE and COMMIT_MESSAGE variables\n",
    "In the following section, we will encode the MESSAGE and COMMIT_MESSAGE variables for the SONAR_ISSUES table and GIT_COMMITS table respectively. For those variables, we will calulate the length of the message for each issue/commit, and reassigning the column with that new value instead of the text from the original message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_length(sonar_issues,\"MESSAGE\")\n",
    "message_length(git_commits,\"COMMIT_MESSAGE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISSUE_CODE_LENGTH variable\n",
    "\n",
    "In the following cells we will proceed to computate the length mean per issue with the START_LINE and END_LINE variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_length = []\n",
    "for index, row in sonar_issues.iterrows():\n",
    "    diff = row['END_LINE'] - row['START_LINE']\n",
    "    issue_length.append(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CREATION_ANALYSIS_KEY</th>\n",
       "      <th>EFFORT</th>\n",
       "      <th>MESSAGE</th>\n",
       "      <th>CLOSE_ANALYSIS_KEY</th>\n",
       "      <th>BLOCKER</th>\n",
       "      <th>CRITICAL</th>\n",
       "      <th>INFO</th>\n",
       "      <th>MAJOR</th>\n",
       "      <th>MINOR</th>\n",
       "      <th>ISSUE_CODE_LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>AWd5_psxC4KKKThc-qK6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>53</td>\n",
       "      <td>AWeDrnWEC4KKKThcAtWf</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>AWd5_psxC4KKKThc-qK6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>53</td>\n",
       "      <td>AWeMr1K3C4KKKThcB9hi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>AWd5_psxC4KKKThc-qK6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>53</td>\n",
       "      <td>AWeMr1K3C4KKKThcB9hi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>AWd5_psxC4KKKThc-qK6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>53</td>\n",
       "      <td>AWeMr1K3C4KKKThcB9hi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>AWd5_psxC4KKKThc-qK6</td>\n",
       "      <td>10.0</td>\n",
       "      <td>53</td>\n",
       "      <td>AWeMr1K3C4KKKThcB9hi</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    CREATION_ANALYSIS_KEY  EFFORT  MESSAGE    CLOSE_ANALYSIS_KEY  BLOCKER  \\\n",
       "41   AWd5_psxC4KKKThc-qK6    10.0       53  AWeDrnWEC4KKKThcAtWf        0   \n",
       "198  AWd5_psxC4KKKThc-qK6    10.0       53  AWeMr1K3C4KKKThcB9hi        0   \n",
       "199  AWd5_psxC4KKKThc-qK6    10.0       53  AWeMr1K3C4KKKThcB9hi        0   \n",
       "200  AWd5_psxC4KKKThc-qK6    10.0       53  AWeMr1K3C4KKKThcB9hi        0   \n",
       "201  AWd5_psxC4KKKThc-qK6    10.0       53  AWeMr1K3C4KKKThcB9hi        0   \n",
       "\n",
       "     CRITICAL  INFO  MAJOR  MINOR  ISSUE_CODE_LENGTH  \n",
       "41          0     1      0      0                0.0  \n",
       "198         0     1      0      0                0.0  \n",
       "199         0     1      0      0                0.0  \n",
       "200         0     1      0      0                0.0  \n",
       "201         0     1      0      0                0.0  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar_issues = sonar_issues.drop('START_LINE', axis=1)\n",
    "sonar_issues = sonar_issues.drop('END_LINE', axis=1)\n",
    "sonar_issues['ISSUE_CODE_LENGTH'] = issue_length\n",
    "sonar_issues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resum:\n",
    "- 66711 rows in sonar analysis\n",
    "- 719186 rows un cop fent el join amb la variable creation_Analysis key\n",
    "- 728409 rows un cop fent el join amb el CLOSE_ANALYSIS_KEY\n",
    "- 1447595 rows dels dos joints \n",
    "- 1200310 unique rows en total dels dos joints\n",
    "- 1197656 rows un cop fent el join del SONRA_MEASURES i SONAR_COMPLETE (sonar_analysis + sonar_issues)\n",
    "- 1083655 unique rows en total dels dos joints\n",
    "\n",
    "(ho he calulat abans amb datafrme.shape[0] però els chuncks els he eliminat per netejar el codi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining SONAR_ANALYSIS with SONAR_ISSUES\n",
    "sonar_complete_1 = pd.merge(sonar_issues, sonar_analysis, left_on='CREATION_ANALYSIS_KEY', right_on='ANALYSIS_KEY', how='inner')\n",
    "sonar_complete_2 = pd.merge(sonar_issues, sonar_analysis, left_on='CLOSE_ANALYSIS_KEY', right_on='ANALYSIS_KEY', how='inner')\n",
    "sonar_complete_1 = sonar_complete_1.drop('CREATION_ANALYSIS_KEY', axis=1)\n",
    "sonar_complete_1 = sonar_complete_1.drop('CLOSE_ANALYSIS_KEY', axis=1)\n",
    "sonar_complete_2 = sonar_complete_2.drop('CREATION_ANALYSIS_KEY', axis=1)\n",
    "sonar_complete_2 = sonar_complete_2.drop('CLOSE_ANALYSIS_KEY', axis=1)\n",
    "\n",
    "sonar_complete = pd.concat([sonar_complete_1, sonar_complete_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deleting duplicated rows\n",
    "sonar_complete = sonar_complete[sonar_complete.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining SONAR_ANALYSIS with SONAR_MEASURES\n",
    "sonar_complete = pd.merge(sonar_complete, sonar_measures, left_on='ANALYSIS_KEY', right_on='analysis_key', how='inner')\n",
    "sonar_complete = sonar_complete.drop('ANALYSIS_KEY', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1083655"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deleting duplicated rows\n",
    "sonar_complete = sonar_complete[sonar_complete.duplicated()]\n",
    "sonar_complete.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_complete =  pd.merge(git_commits, git_commits_changes, left_on='COMMIT_HASH', right_on='COMMIT_HASH', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exactly one of npartitions and chunksize must be specified.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-86-e46b1d84d280>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdd_git_complete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgit_complete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m '''\n\u001b[1;32m      3\u001b[0m \u001b[0mfinal_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgit_complete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msonar_complete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'COMMIT_HASH'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright_on\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'REVISION'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'inner'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mfinal_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m '''\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/dask/dataframe/io/io.py\u001b[0m in \u001b[0;36mfrom_pandas\u001b[0;34m(data, npartitions, chunksize, sort, name)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnpartitions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exactly one of npartitions and chunksize must be specified.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Exactly one of npartitions and chunksize must be specified."
     ]
    }
   ],
   "source": [
    "dd_git_complete = dd.from_pandas(git_complete)\n",
    "'''\n",
    "final_table = pd.merge(git_complete, sonar_complete, left_on='COMMIT_HASH', right_on='REVISION', how='inner')\n",
    "final_table.shape[0]\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
